{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3070</td><td>application_1513605045578_0305</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1513605045578_0305/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop20:8042/node/containerlogs/container_e28_1513605045578_0305_01_000001/har_1__kimham00\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "def map_fun(args, ctx):\n",
    "    \"\"\"Training/Inference Function executed by parameter-servers and workers in distributed TFOS\"\"\"\n",
    "    from tensorflowonspark import TFNode\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import time\n",
    "    from hops import tensorboard\n",
    "    from datetime import datetime\n",
    "    import pydoop.hdfs as pyhdfs\n",
    "    startTime= datetime.now()\n",
    "\n",
    "    # Constants\n",
    "    NUM_FEATURES = 3\n",
    "    NUM_CLASSES = 7\n",
    "    SEQUENCE_SIZE = 200\n",
    "    NUM_HIDDEN_UNITS = 64\n",
    "    TEST_SIZE = 130622\n",
    "    TRAIN_SIZE = 522490\n",
    "    LEARNING_RATE = args.learningrate\n",
    "\n",
    "    def print_log(worker_num, arg):\n",
    "        print(\"Worker {0}: {1}\".format(worker_num, arg))\n",
    "\n",
    "    # Cluster parameters\n",
    "    worker_num = ctx.worker_num\n",
    "    job_name = ctx.job_name\n",
    "    task_index = ctx.task_index\n",
    "    cluster_spec = ctx.cluster_spec\n",
    "    print_log(worker_num, \"task_index: {0}, job_name {1}, cluster_spec: {2}\".format(task_index, job_name, cluster_spec))\n",
    "    num_workers = len(cluster_spec['worker'])\n",
    "\n",
    "    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n",
    "    if job_name == \"ps\":\n",
    "        time.sleep((worker_num + 1) * 5)\n",
    "\n",
    "    batch_size = 1024\n",
    "    print_log(worker_num, \"batch_size: {0}\".format(batch_size))\n",
    "\n",
    "    # Get TF cluster and server instances\n",
    "    cluster, server = TFNode.start_cluster_server(ctx, 1, args.rdma)\n",
    "\n",
    "    def read_csv_features(feature_dir, batch_size=100, num_epochs=None, task_index=None, num_workers=None):\n",
    "        \"\"\" Reads pre-processed and parallelized CSV files from disk into TF-HDFS queues\"\"\"\n",
    "        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n",
    "\n",
    "        # Setup queue of csv feature filenames\n",
    "        tf_record_pattern = os.path.join(feature_dir, 'part-*')\n",
    "        features = tf.gfile.Glob(tf_record_pattern)\n",
    "        print_log(worker_num, \"features: {0}\".format(features))\n",
    "        feature_queue = tf.train.string_input_producer(features, shuffle=False, capacity=1000, num_epochs=num_epochs,\n",
    "                                                       name=\"feature_queue\")\n",
    "        # Setup reader for feature queue\n",
    "        feature_reader = tf.TextLineReader(name=\"feature_reader\")\n",
    "        _, feat_csv = feature_reader.read(feature_queue)\n",
    "        feature_defaults = [[1.0] for col in range(SEQUENCE_SIZE * NUM_FEATURES)]\n",
    "        feature = tf.stack(tf.decode_csv(feat_csv, feature_defaults), name=\"input_features\")\n",
    "        print_log(worker_num, \"features: {0}, shape: {1}\".format(feature, feature.shape))\n",
    "\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([feature], batch_size, num_threads=1, name=\"batch_csv\")\n",
    "\n",
    "    def read_csv_labels(label_dir, batch_size=10, num_epochs=None, task_index=None, num_workers=None):\n",
    "        \"\"\" Reads pre-processed and parallelized CSV files from disk into TF-HDFS queues\"\"\"\n",
    "        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n",
    "\n",
    "        # Setup queue of csv label filenames\n",
    "        tf_record_pattern = os.path.join(label_dir, 'part-*')\n",
    "        labels = tf.gfile.Glob(tf_record_pattern)\n",
    "        print_log(worker_num, \"labels: {0}\".format(labels))\n",
    "        label_queue = tf.train.string_input_producer(labels, shuffle=False, capacity=1000, num_epochs=num_epochs,\n",
    "                                                     name=\"label_queue\")\n",
    "        # Setup reader for label queue\n",
    "        label_reader = tf.TextLineReader(name=\"label_reader\")\n",
    "        _, label_csv = label_reader.read(label_queue)\n",
    "        label_defaults = [tf.constant([], dtype=tf.int64)]\n",
    "        label = tf.stack(tf.decode_csv(label_csv, label_defaults), name=\"input_labels\")\n",
    "        print_log(worker_num, tf.shape(label))\n",
    "        print_log(worker_num, \"label: {0}\".format(label))\n",
    "\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([label], batch_size, num_threads=1, name=\"label_batch_csv\")\n",
    "\n",
    "    if job_name == \"ps\":\n",
    "        print_log(worker_num, \"Parameter Server Joining\")\n",
    "        server.join()\n",
    "\n",
    "    elif job_name == \"worker\":\n",
    "        print_log(worker_num, \"worker {0} starting\")\n",
    "\n",
    "        with tf.device(tf.train.replica_device_setter(\n",
    "                worker_device=\"/job:worker/task:%d\" % task_index,\n",
    "                cluster=cluster)):\n",
    "\n",
    "            ################  The computational graph ########################\n",
    "\n",
    "            # Queue parameters\n",
    "            num_epochs = 1 if args.mode == \"inference\" else None if args.epochs == 0 else args.epochs\n",
    "            index = task_index if args.mode == \"inference\" else None\n",
    "            workers = num_workers if args.mode == \"inference\" else None\n",
    "\n",
    "            # Placeholders or QueueRunner/Readers for input data\n",
    "            features = TFNode.hdfs_path(ctx, args.features)  # input csv files\n",
    "            labels = TFNode.hdfs_path(ctx, args.labels)  # input csv files\n",
    "            test_features = TFNode.hdfs_path(ctx, args.testfeatures)  # input csv files\n",
    "            test_labels = TFNode.hdfs_path(ctx, args.testlabels)  # input csv files\n",
    "\n",
    "            # Read input from queues\n",
    "            x = read_csv_features(features, batch_size, num_epochs, index, workers)\n",
    "            x = tf.reshape(x, [x.shape[0].value, SEQUENCE_SIZE, NUM_FEATURES])\n",
    "            y = read_csv_labels(labels, batch_size, num_epochs, index, workers)\n",
    "            x_test = read_csv_features(test_features, batch_size, num_epochs, index, workers)\n",
    "            x_test = tf.reshape(x_test, [x_test.shape[0].value, SEQUENCE_SIZE, NUM_FEATURES])\n",
    "            y_test = read_csv_labels(test_labels, batch_size, num_epochs, index, workers)\n",
    "\n",
    "            # First FCC layer\n",
    "            W = {\n",
    "                'hidden': tf.Variable(tf.random_normal([NUM_FEATURES, NUM_HIDDEN_UNITS])),\n",
    "                'output': tf.Variable(tf.random_normal([NUM_HIDDEN_UNITS, NUM_CLASSES]))\n",
    "            }\n",
    "            biases = {\n",
    "                'hidden': tf.Variable(tf.random_normal([NUM_HIDDEN_UNITS], mean=1.0)),\n",
    "                'output': tf.Variable(tf.random_normal([NUM_CLASSES]))\n",
    "            }\n",
    "\n",
    "            # Reshape for convenience\n",
    "            X = tf.transpose(x, [1, 0, 2])\n",
    "            X = tf.reshape(X, [-1, NUM_FEATURES])\n",
    "            X_test = tf.transpose(x_test, [1, 0, 2])\n",
    "            X_test = tf.reshape(X_test, [-1, NUM_FEATURES])\n",
    "\n",
    "            # Output from first FCC layer, split into sequences for truncated backprop\n",
    "            hidden = tf.nn.relu(tf.matmul(X, W['hidden']) + biases['hidden'])\n",
    "            hidden = tf.split(hidden, SEQUENCE_SIZE, 0)\n",
    "            hidden_test = tf.nn.relu(tf.matmul(X_test, W['hidden']) + biases['hidden'])\n",
    "            hidden_test = tf.split(hidden_test, SEQUENCE_SIZE, 0)\n",
    "\n",
    "            # Stack 2 LSTM layers\n",
    "            lstm_layers = [tf.contrib.rnn.BasicLSTMCell(NUM_HIDDEN_UNITS, forget_bias=1.0) for _ in range(2)]\n",
    "            lstm_layers = tf.contrib.rnn.MultiRNNCell(lstm_layers)\n",
    "\n",
    "            # Get output from LSTM layers\n",
    "            outputs, _ = tf.contrib.rnn.static_rnn(lstm_layers, hidden, dtype=tf.float32)\n",
    "            outputs_test, _ = tf.contrib.rnn.static_rnn(lstm_layers, hidden_test, dtype=tf.float32)\n",
    "\n",
    "            # Get output for the last time step\n",
    "            lstm_last_output = outputs[-1]\n",
    "            lstm_last_output_test = outputs_test[-1]\n",
    "\n",
    "            # Output from second FCC layer, aka logits\n",
    "            logits = tf.matmul(lstm_last_output, W['output']) + biases['output']\n",
    "            logits_test = tf.matmul(lstm_last_output_test, W['output']) + biases['output']\n",
    "\n",
    "            # Global step to keep track of how long training have proceeded\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "            # Make predictions with softmax + argmax\n",
    "            softmax_prediction = tf.nn.softmax(logits, name=\"prediction\")\n",
    "            prediction = tf.argmax(softmax_prediction, 1)\n",
    "            softmax_prediction_test = tf.nn.softmax(logits_test, name=\"prediction_test\")\n",
    "            prediction_test = tf.argmax(softmax_prediction_test, 1)\n",
    "\n",
    "            # L2 Regularization\n",
    "            L2_LOSS = 0.0015\n",
    "            l2 = L2_LOSS * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "\n",
    "            # Cross entropy loss + L2 regularization\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    labels=tf.one_hot(tf.reshape(y, [-1]), NUM_CLASSES),\n",
    "                    logits=logits))\n",
    "            loss_test = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    labels=tf.one_hot(tf.reshape(y_test, [-1]), NUM_CLASSES),\n",
    "                    logits=logits_test))\n",
    "            loss_reg = loss + l2\n",
    "            tf.summary.scalar(\"loss_test\", loss_test)  # for tensorboard\n",
    "\n",
    "            # Define optimizer\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(\n",
    "                loss_reg,\n",
    "                global_step=global_step)\n",
    "\n",
    "            # Test trained model\n",
    "            correct_prediction = tf.equal(prediction, tf.argmax(tf.one_hot(tf.reshape(y, [-1]), NUM_CLASSES), 1))\n",
    "            correct_prediction_test = tf.equal(prediction_test, tf.argmax(tf.one_hot(tf.reshape(y_test, [-1]), NUM_CLASSES),1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "            accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32), name=\"accuracy_test\")\n",
    "            tf.summary.scalar(\"acc_test\", accuracy_test)  # for tensorboard\n",
    "\n",
    "            # Utility stuff tensorboard and logging\n",
    "            saver = tf.train.Saver()\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            logdir = tensorboard.logdir()\n",
    "            display_step = 200\n",
    "            history = dict(test_loss=[], test_acc=[], log = [])\n",
    "            print_log(worker_num, \"tensorflow model path: {0}\".format(logdir))\n",
    "\n",
    "            # Setup Supervisor\n",
    "            if job_name == \"worker\" and task_index == 0:\n",
    "                summary_writer = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "\n",
    "            if args.mode == \"train\":\n",
    "                sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                         logdir=logdir,\n",
    "                                         init_op=init_op,\n",
    "                                         summary_op=None,\n",
    "                                         summary_writer=None,\n",
    "                                         saver=saver,\n",
    "                                         global_step=global_step,\n",
    "                                         stop_grace_secs=300,\n",
    "                                         save_model_secs=10)\n",
    "            else:\n",
    "                sv = tf.train.Supervisor(is_chief=(task_index == 0),\n",
    "                                         logdir=logdir,\n",
    "                                         summary_op=None,\n",
    "                                         saver=saver,\n",
    "                                         global_step=global_step,\n",
    "                                         stop_grace_secs=300,\n",
    "                                         save_model_secs=0)\n",
    "\n",
    "    # Run the graph for NUM_STEPS, compute test accuracy incrementally\n",
    "    # The supervisor takes care of session initialization, restoring from\n",
    "    # a checkpoint, and closing when done or an error occurs.\n",
    "    with sv.managed_session(server.target) as sess:\n",
    "        \"\"\"Asynchronous SGD training with supervisor\"\"\"\n",
    "        print_log(worker_num, \"session ready, starting training\")\n",
    "        step = 0\n",
    "        chief_count=0\n",
    "        while not sv.should_stop() and step < args.steps:\n",
    "            if args.mode == \"train\":\n",
    "                _, summary, step = sess.run([train_step, summary_op, global_step])\n",
    "                if(step % 50 == 0):\n",
    "                    print(\"step: {0}\".format(step))\n",
    "                if sv.is_chief and chief_count % (display_step//num_workers) == 0:\n",
    "                    test_a, test_l = sess.run([accuracy_test, loss_test])\n",
    "                    result = \"step: {0}, test acc: {1}, test loss: {2}\".format(step, test_a, test_l)\n",
    "                    history['test_loss'].append(test_l)\n",
    "                    history['test_acc'].append(test_a)\n",
    "                    history['log'].append(result)\n",
    "                    print_log(worker_num, result)\n",
    "                    summary_writer.add_summary(summary, step)\n",
    "                if sv.is_chief:\n",
    "                    chief_count = chief_count+1\n",
    "            else:\n",
    "                print_log(worker_num, \"inference not supported, do it locally with saved model instead\")\n",
    "\n",
    "        if args.mode == \"inference\":\n",
    "            print_log(worker_num, \"inference not supported, do it locally with saved model instead\")\n",
    "\n",
    "        if sv.is_chief:\n",
    "            print_log(worker_num, \"Saving session stats\")\n",
    "            endTime = datetime.now()\n",
    "            timeElapsed= endTime-startTime\n",
    "            accs = \"\\n\".join(str(x) for x in history[\"test_acc\"])\n",
    "            loss = \"\\n\".join(str(x) for x in history[\"test_loss\"])\n",
    "            logs = \"\\n\".join(str(x) for x in history[\"log\"])\n",
    "            pyhdfs.dump(accs, args.output + \"/accuracy\")\n",
    "            pyhdfs.dump(loss, args.output + \"/loss\")\n",
    "            pyhdfs.dump(logs, args.output + \"/log\")\n",
    "            time = \"start: \" + str(startTime) + \"\\nend: \" + str(endTime) + \"\\nduration: \" + str(timeElapsed)\n",
    "            pyhdfs.dump(time, args.output + \"/time\")\n",
    "            summ = tf.summary.FileWriter(logdir, graph=tf.get_default_graph())\n",
    "            summ.flush()\n",
    "\n",
    "        # Ask for all the services to stop.\n",
    "        print(\"{0} stopping supervisor\".format(datetime.now().isoformat()))\n",
    "        sv.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spark Cluster Setup For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from hops import util\n",
    "from hops import hdfs\n",
    "\n",
    "from tensorflowonspark import TFCluster\n",
    "\n",
    "project_path = \"/Projects/\" + hdfs.project_name()\n",
    "\n",
    "TRAIN_FEATURES_PATH = project_path + \"/HAR_Dataset/cleaned_data_parallel/train/features\"\n",
    "TRAIN_LABELS_PATH = project_path + \"/HAR_Dataset/cleaned_data_parallel/train/labels\"\n",
    "TEST_FEATURES_PATH = project_path + \"/HAR_Dataset/cleaned_data_parallel/test/features\"\n",
    "TEST_LABELS_PATH = project_path + \"/HAR_Dataset/cleaned_data_parallel/test/labels\"\n",
    "OUTPUT_PATH = project_path + \"/HAR_Dataset/output/dist_async\"\n",
    "\n",
    "sc = spark.sparkContext\n",
    "num_executors = util.num_executors(spark)\n",
    "num_ps = util.num_param_servers(spark)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-e\", \"--epochs\", help=\"number of epochs\", type=int, default=1000000)\n",
    "parser.add_argument(\"-f\", \"--format\", help=\"example format: (csv|pickle|tfr)\", choices=[\"csv\",\"pickle\",\"tfr\"], default=\"csv\")\n",
    "parser.add_argument(\"-fe\", \"--features\", help=\"HDFS path to MNIST images in parallelized format\", default=TRAIN_FEATURES_PATH)\n",
    "parser.add_argument(\"-l\", \"--labels\", help=\"HDFS path to MNIST labels in parallelized format\", default=TRAIN_LABELS_PATH)\n",
    "parser.add_argument(\"-tf\", \"--testfeatures\", help=\"HDFS path to features in parallelized format\", default=TEST_FEATURES_PATH)\n",
    "parser.add_argument(\"-tl\", \"--testlabels\", help=\"HDFS path to labels in parallelized format\", default=TEST_LABELS_PATH)\n",
    "parser.add_argument(\"-m\", \"--model\", help=\"HDFS path to save/load model during train/inference\", default=OUTPUT_PATH)\n",
    "parser.add_argument(\"-n\", \"--cluster_size\", help=\"number of nodes in the cluster (for Spark Standalone)\", type=int, default=num_executors)\n",
    "parser.add_argument(\"-o\", \"--output\", help=\"HDFS path to save test/inference output\", default=OUTPUT_PATH)\n",
    "parser.add_argument(\"-r\", \"--readers\", help=\"number of reader/enqueue threads\", type=int, default=1)\n",
    "parser.add_argument(\"-s\", \"--steps\", help=\"maximum number of steps\", type=int, default=10001)\n",
    "parser.add_argument(\"-tb\", \"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\n",
    "parser.add_argument(\"-X\", \"--mode\", help=\"train|inference\", default=\"train\")\n",
    "parser.add_argument(\"-c\", \"--rdma\", help=\"use rdma connection\", default=False)\n",
    "parser.add_argument(\"-lr\", \"--learningrate\", help=\"number of epochs\", type=float, default=0.00025)\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(\"{0} ===== Start\".format(datetime.now().isoformat()))\n",
    "\n",
    "cluster = TFCluster.run(sc, map_fun, args, args.cluster_size, num_ps, args.tensorboard, TFCluster.InputMode.TENSORFLOW)\n",
    "cluster.shutdown()\n",
    "\n",
    "print(\"{0} ===== Stop\".format(datetime.now().isoformat()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
