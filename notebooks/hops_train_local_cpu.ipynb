{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper function for distributed experiments with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def wrapper(learning_rate):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from hops import tensorboard\n",
    "    from hops import hdfs\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import pydoop.hdfs as pyhdfs\n",
    "    startTime= datetime.now()\n",
    "\n",
    "    tensorboard_logdir = tensorboard.logdir()\n",
    "    project_path = hdfs.project_path()\n",
    "    \n",
    "    # Constants\n",
    "    NUM_STEPS = 2001\n",
    "    BATCH_SIZE = 1024\n",
    "    TEST_SIZE = 130622\n",
    "    TRAIN_SIZE= 522490\n",
    "    NUM_FEATURES = 3\n",
    "    NUM_CLASSES = 7\n",
    "    SEQUENCE_SIZE = 200\n",
    "    NUM_HIDDEN_UNITS = 64\n",
    "    NUM_EPOCHS = 100\n",
    "    TRAIN_FEATURES_PATH = project_path + \"/HAR_Dataset/cleaned_data_parallel/train/features\"\n",
    "    TRAIN_LABELS_PATH = project_path + \"/HAR_Dataset/cleaned_data_parallel/train/labels\"\n",
    "    TEST_FEATURES_PATH = project_path + \"/HAR_Dataset/cleaned_data_parallel/test/features\"\n",
    "    TEST_LABELS_PATH = project_path + \"/HAR_Dataset/cleaned_data_parallel/test/labels\"\n",
    "    OUTPUT_PATH = project_path + \"HAR_Dataset/output/local_cpu\"\n",
    "\n",
    "    def read_csv_features(feature_dir, batch_size=100, num_epochs=None, task_index=None, num_workers=None):\n",
    "        \"\"\" Reads pre-processed and parallelized CSV files from disk into TF-HDFS queues\"\"\"\n",
    "        # Setup queue of csv feature filenames\n",
    "        tf_record_pattern = os.path.join(feature_dir, 'part-*')\n",
    "        features = tf.gfile.Glob(tf_record_pattern)\n",
    "        feature_queue = tf.train.string_input_producer(features, shuffle=False, capacity=1000, num_epochs=num_epochs,\n",
    "                                                       name=\"feature_queue\")\n",
    "        # Setup reader for feature queue\n",
    "        feature_reader = tf.TextLineReader(name=\"feature_reader\")\n",
    "        _, feat_csv = feature_reader.read(feature_queue)\n",
    "        feature_defaults = [[1.0] for col in range(SEQUENCE_SIZE * NUM_FEATURES)]\n",
    "        feature = tf.stack(tf.decode_csv(feat_csv, feature_defaults), name=\"input_features\")\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([feature], batch_size, num_threads=1, name=\"batch_csv\")\n",
    "\n",
    "    def read_csv_labels(label_dir, batch_size=10, num_epochs=None, task_index=None, num_workers=None):\n",
    "        \"\"\" Reads pre-processed and parallelized CSV files from disk into TF-HDFS queues\"\"\"\n",
    "        # Setup queue of csv label filenames\n",
    "        tf_record_pattern = os.path.join(label_dir, 'part-*')\n",
    "        labels = tf.gfile.Glob(tf_record_pattern)\n",
    "        label_queue = tf.train.string_input_producer(labels, shuffle=False, capacity=1000, num_epochs=num_epochs,\n",
    "                                                     name=\"label_queue\")\n",
    "        # Setup reader for label queue\n",
    "        label_reader = tf.TextLineReader(name=\"label_reader\")\n",
    "        _, label_csv = label_reader.read(label_queue)\n",
    "        label_defaults = [tf.constant([], dtype=tf.int64)]\n",
    "        label = tf.stack(tf.decode_csv(label_csv, label_defaults), name=\"input_labels\")\n",
    "        # Return a batch of examples\n",
    "        return tf.train.batch([label], batch_size, num_threads=1, name=\"label_batch_csv\")\n",
    "\n",
    "    ################  The computational graph ########################\n",
    "    \n",
    "    #Read input from queues\n",
    "    x_train_csv = read_csv_features(TRAIN_FEATURES_PATH, BATCH_SIZE)\n",
    "    y_train_csv = read_csv_labels(TRAIN_LABELS_PATH, BATCH_SIZE)\n",
    "    x_test_csv = read_csv_features(TEST_FEATURES_PATH, TEST_SIZE)\n",
    "    y_test_csv = read_csv_labels(TEST_LABELS_PATH, TEST_SIZE)\n",
    "    x_train_csv = tf.reshape(x_train_csv, [x_train_csv.shape[0].value, SEQUENCE_SIZE, NUM_FEATURES])\n",
    "    x_test_csv = tf.reshape(x_test_csv, [x_test_csv.shape[0].value, SEQUENCE_SIZE, NUM_FEATURES])\n",
    "\n",
    "    # First FCC layer\n",
    "    W = {\n",
    "        'hidden': tf.Variable(tf.random_normal([NUM_FEATURES, NUM_HIDDEN_UNITS])),\n",
    "        'output': tf.Variable(tf.random_normal([NUM_HIDDEN_UNITS, NUM_CLASSES]))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([NUM_HIDDEN_UNITS], mean=1.0)),\n",
    "        'output': tf.Variable(tf.random_normal([NUM_CLASSES]))\n",
    "    }\n",
    "    \n",
    "    # Reshape for convenience\n",
    "    X = tf.transpose(x_train_csv, [1, 0, 2])\n",
    "    X = tf.reshape(X, [-1, NUM_FEATURES])\n",
    "    X_test = tf.transpose(x_test_csv, [1, 0, 2])\n",
    "    X_test = tf.reshape(X_test, [-1, NUM_FEATURES])\n",
    "    \n",
    "    # Output from first FCC layer, split into sequences for truncated backprop\n",
    "    hidden = tf.nn.relu(tf.matmul(X, W['hidden']) + biases['hidden'])\n",
    "    hidden = tf.split(hidden, SEQUENCE_SIZE, 0)\n",
    "    hidden_test = tf.nn.relu(tf.matmul(X_test, W['hidden']) + biases['hidden'])\n",
    "    hidden_test = tf.split(hidden_test, SEQUENCE_SIZE, 0)\n",
    "\n",
    "    # Stack 2 LSTM layers\n",
    "    lstm_layers = [tf.contrib.rnn.BasicLSTMCell(NUM_HIDDEN_UNITS, forget_bias=1.0) for _ in range(2)]\n",
    "    lstm_layers = tf.contrib.rnn.MultiRNNCell(lstm_layers)\n",
    "\n",
    "    # Get output from LSTM layers\n",
    "    outputs, _ = tf.contrib.rnn.static_rnn(lstm_layers, hidden, dtype=tf.float32)\n",
    "    outputs_test, _ = tf.contrib.rnn.static_rnn(lstm_layers, hidden_test, dtype=tf.float32)\n",
    "    \n",
    "    # Get output for the last time step\n",
    "    lstm_last_output = outputs[-1]\n",
    "    lstm_last_output_test = outputs_test[-1]\n",
    "\n",
    "    # Get Logits\n",
    "    logits = tf.matmul(lstm_last_output, W['output']) + biases['output']\n",
    "    logits_test = tf.matmul(lstm_last_output_test, W['output']) + biases['output']\n",
    "\n",
    "    # Global step to keep track of how long training have proceeded,\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "    \n",
    "    # Make predictions with softmax + argmax\n",
    "    softmax_prediction = tf.nn.softmax(logits, name=\"prediction\")\n",
    "    prediction = tf.argmax(softmax_prediction, 1)\n",
    "    softmax_prediction_test = tf.nn.softmax(logits_test, name=\"prediction_test\")\n",
    "    prediction_test = tf.argmax(softmax_prediction_test, 1)\n",
    "\n",
    "    #L2 Regularization\n",
    "    L2_LOSS = 0.0015\n",
    "    l2 = L2_LOSS * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "\n",
    "    # Cross entropy loss + L2 regularization\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.one_hot(tf.reshape(y_train_csv, [-1]), NUM_CLASSES),\n",
    "            logits=logits))\n",
    "    loss_test = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.one_hot(tf.reshape(y_test_csv, [-1]), NUM_CLASSES),\n",
    "            logits=logits_test))\n",
    "    loss_reg = loss + l2\n",
    "\n",
    "    tf.summary.scalar(\"loss_test\", loss_test) #for tensorboard\n",
    "\n",
    "    # Define optimizer\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n",
    "            loss_reg,\n",
    "            global_step=global_step)\n",
    "\n",
    "    # Test trained model\n",
    "    correct_prediction = tf.equal(prediction, tf.argmax(tf.one_hot(tf.reshape(y_train_csv, [-1]), NUM_CLASSES),1))\n",
    "    correct_prediction_test = tf.equal(prediction_test, tf.argmax(tf.one_hot(tf.reshape(y_test_csv, [-1]), NUM_CLASSES),1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "    accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32), name=\"accuracy_test\")\n",
    "    tf.summary.scalar(\"acc_test\", accuracy_test) #for tensorboard\n",
    "\n",
    "    # Utility stuff tensorboard and logging\n",
    "    saver = tf.train.Saver()\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    summary_writer = tf.summary.FileWriter(tensorboard_logdir, graph=tf.get_default_graph())\n",
    "    display_step = 200\n",
    "    history = dict(test_loss=[], test_acc=[], log = [])\n",
    "    \n",
    "    # Run the graph for NUM_STEPS, compute test accuracy incrementally\n",
    "    with tf.Session() as sess:\n",
    "        print('Initialzing training, first step takes some time to compute the accuracy...')\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        for i in range(NUM_STEPS):\n",
    "            if coord.should_stop():\n",
    "                print(\"coord stop\")\n",
    "                break\n",
    "            else:\n",
    "                batch_x, batch_y = sess.run([x_train_csv, y_train_csv])\n",
    "                sess.run(train_step, feed_dict={x_train_csv: batch_x, y_train_csv: batch_y})\n",
    "                if(i % 50 == 0):\n",
    "                    print(\"step: {0}\".format(i))\n",
    "                if (i % display_step == 0):\n",
    "                    eval_x, eval_y = sess.run([x_test_csv, y_test_csv])\n",
    "                    summary, step, test_a, test_l = sess.run([summary_op, global_step, accuracy_test, loss_test], feed_dict={x_test_csv: eval_x, y_test_csv: eval_y})\n",
    "                    result = \"step: {0}, test acc: {1}, test loss: {2}\".format(step, test_a, test_l)\n",
    "                    print(result)\n",
    "                    hdfs.log(result)\n",
    "                    history['test_loss'].append(test_a)\n",
    "                    history['test_acc'].append(test_l)\n",
    "                    history['log'].append(result)\n",
    "                    summary_writer.add_summary(summary, step)\n",
    "                    \n",
    "        endTime = datetime.now()\n",
    "        timeElapsed= endTime-startTime                     \n",
    "        accs = \"\\n\".join(str(x) for x in history[\"test_acc\"])\n",
    "        loss = \"\\n\".join(str(x) for x in history[\"test_loss\"])\n",
    "        logs = \"\\n\".join(str(x) for x in history[\"log\"])\n",
    "        pyhdfs.dump(accs, OUTPUT_PATH + \"/accuracy\")\n",
    "        pyhdfs.dump(loss, OUTPUT_PATH + \"/loss\")\n",
    "        pyhdfs.dump(logs, OUTPUT_PATH + \"/log\")\n",
    "        time = \"start: \" + str(startTime) + \"\\nend: \" + str(endTime) + \"\\nduration: \" + str(timeElapsed)\n",
    "        pyhdfs.dump(time, OUTPUT_PATH + \"/time\")\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from hops import util\n",
    "from hops import tflauncher\n",
    "\n",
    "# Define dict for hyperparameters\n",
    "args_dict = {'learning_rate': [0.0025]}\n",
    "\n",
    "# Generate a grid for the given hyperparameters\n",
    "args_dict_grid = util.grid_params(args_dict)\n",
    "\n",
    "# Launch training\n",
    "tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper, args_dict_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from hops import tensorboard\n",
    "\n",
    "tensorboard.visualize(spark, tensorboard_hdfs_logdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
