{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper function for distributed experiments with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n",
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full',)).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3236</td><td>application_1513605045578_0477</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1513605045578_0477/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop6:8042/node/containerlogs/container_e28_1513605045578_0477_01_000001/har_2__kimham00\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "def wrapper(learning_rate):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from hops import tensorboard\n",
    "    from hops import hdfs\n",
    "    from hops import devices\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import pydoop.hdfs as pyhdfs\n",
    "    startTime = datetime.now()\n",
    "\n",
    "    tensorboard_logdir = tensorboard.logdir()\n",
    "    project_path = hdfs.project_path()\n",
    "    num_gpus = devices.get_num_gpus()\n",
    "    print(\"Num gpus: {0}\".format(num_gpus))\n",
    "    print(\"Cuda support: {0}\".format(tf.test.is_built_with_cuda()))\n",
    "    # Constants\n",
    "    NUM_STEPS = 2001\n",
    "    BATCH_SIZE = 1024\n",
    "    TEST_SIZE = 130622\n",
    "    TRAIN_SIZE = 522490\n",
    "    NUM_FEATURES = 3\n",
    "    NUM_CLASSES = 7\n",
    "    SEQUENCE_SIZE = 200\n",
    "    NUM_HIDDEN_UNITS = 64\n",
    "    NUM_EPOCHS = 100\n",
    "    TRAIN_FEATURES_PATH = project_path + \"HAR_Dataset/cleaned_data/train/features/x_train.csv\"\n",
    "    TRAIN_LABELS_PATH = project_path + \"HAR_Dataset/cleaned_data/train/labels/y_train.csv\"\n",
    "    TEST_FEATURES_PATH = project_path + \"HAR_Dataset/cleaned_data/test/features/x_test.csv\"\n",
    "    TEST_LABELS_PATH = project_path + \"HAR_Dataset/cleaned_data/test/labels/y_test.csv\"\n",
    "    OUTPUT_PATH = project_path + \"HAR_Dataset/output/local_gpu\"\n",
    "\n",
    "    # Build the function to average the gradients\n",
    "    def average_gradients(tf, tower_grads):\n",
    "        average_grads = []\n",
    "        for grad_and_vars in zip(*tower_grads):\n",
    "            # Note that each grad_and_vars looks like the following:\n",
    "            #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "            grads = []\n",
    "            for g, _ in grad_and_vars:\n",
    "                # Add 0 dimension to the gradients to represent the tower.\n",
    "                expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "                # Append on a 'tower' dimension which we will average over below.\n",
    "                grads.append(expanded_g)\n",
    "\n",
    "            # Average over the 'tower' dimension.\n",
    "            grad = tf.concat(grads, 0)\n",
    "            grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "            # Keep in mind that the Variables are redundant because they are shared\n",
    "            # across towers. So .. we will just return the first tower's pointer to\n",
    "            # the Variable.\n",
    "            v = grad_and_vars[0][1]\n",
    "            grad_and_var = (grad, v)\n",
    "            average_grads.append(grad_and_var)\n",
    "        return average_grads\n",
    "\n",
    "    def read_csv_features(features, batch_size=100):\n",
    "        \"\"\" Reads CSV files from disk into TF-HDFS queues and return batch\"\"\"\n",
    "        feature_queue = tf.train.string_input_producer(features, shuffle=False, capacity=100000, name=\"feature_queue\")\n",
    "        feature_reader = tf.TextLineReader(name=\"feature_reader\")\n",
    "        _, feat_csv = feature_reader.read(feature_queue)\n",
    "        feature_defaults = [[1.0] for col in range(SEQUENCE_SIZE * NUM_FEATURES)]\n",
    "        feature = tf.stack(tf.decode_csv(feat_csv, feature_defaults), name=\"input_features\")\n",
    "        print(\"features: {0}, shape: {1}\".format(feature, feature.shape))\n",
    "        return tf.train.batch([feature], batch_size, num_threads=1, name=\"batch_csv\", capacity=100000)\n",
    "\n",
    "    def read_csv_labels(labels, batch_size=10):\n",
    "        \"\"\"Reads CSV files from disk into TF-HDFS queues and return batch\"\"\"\n",
    "        label_queue = tf.train.string_input_producer(labels, shuffle=False, capacity=100000, name=\"label_queue\")\n",
    "        label_reader = tf.TextLineReader(name=\"label_reader\")\n",
    "        _, label_csv = label_reader.read(label_queue)\n",
    "        label_defaults = [tf.constant([], dtype=tf.int64)]\n",
    "        label = tf.stack(tf.decode_csv(label_csv, label_defaults), name=\"input_labels\")\n",
    "        return tf.train.batch([label], batch_size, num_threads=1, name=\"label_batch_csv\", capacity=100000)\n",
    "\n",
    "    ################  The computational graph ########################\n",
    "\n",
    "    # Place all ops on CPU by default\n",
    "    with tf.device('/cpu:0'):\n",
    "        tower_grads = []\n",
    "        reuse_vars = False\n",
    "\n",
    "        # Read input from queues\n",
    "        x_train_csv = read_csv_features([TRAIN_FEATURES_PATH], BATCH_SIZE)\n",
    "        y_train_csv = read_csv_labels([TRAIN_LABELS_PATH], BATCH_SIZE)\n",
    "        x_test_csv = read_csv_features([TEST_FEATURES_PATH], TEST_SIZE)\n",
    "        y_test_csv = read_csv_labels([TEST_LABELS_PATH], TEST_SIZE)\n",
    "        x_train_csv = tf.reshape(x_train_csv, [x_train_csv.shape[0].value, SEQUENCE_SIZE, NUM_FEATURES])\n",
    "        x_test_csv = tf.reshape(x_test_csv, [x_test_csv.shape[0].value, SEQUENCE_SIZE, NUM_FEATURES])\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        \n",
    "        # Loop over all GPUs and construct their own computation graph\n",
    "        for i in range(num_gpus):\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                # Split data between GPUs\n",
    "                _x = x_train_csv[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "                _y = y_train_csv[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "\n",
    "                # First FCC layer\n",
    "                W = {\n",
    "                    'hidden': tf.Variable(tf.random_normal([NUM_FEATURES, NUM_HIDDEN_UNITS])),\n",
    "                    'output': tf.Variable(tf.random_normal([NUM_HIDDEN_UNITS, NUM_CLASSES]))\n",
    "                }\n",
    "                biases = {\n",
    "                    'hidden': tf.Variable(tf.random_normal([NUM_HIDDEN_UNITS], mean=1.0)),\n",
    "                    'output': tf.Variable(tf.random_normal([NUM_CLASSES]))\n",
    "                }\n",
    "\n",
    "                # Reshape for convenience\n",
    "                X = tf.transpose(_x, [1, 0, 2])\n",
    "                X = tf.reshape(X, [-1, NUM_FEATURES])\n",
    "                X_test = tf.transpose(x_test_csv, [1, 0, 2])\n",
    "                X_test = tf.reshape(X_test, [-1, NUM_FEATURES])\n",
    "\n",
    "                # Output from first FCC layer, split into sequences for truncated backprop\n",
    "                hidden = tf.nn.relu(tf.matmul(X, W['hidden']) + biases['hidden'])\n",
    "                hidden = tf.split(hidden, SEQUENCE_SIZE, 0)\n",
    "                hidden_test = tf.nn.relu(tf.matmul(X_test, W['hidden']) + biases['hidden'])\n",
    "                hidden_test = tf.split(hidden_test, SEQUENCE_SIZE, 0)\n",
    "\n",
    "                # Stack 2 LSTM layers\n",
    "                lstm_layers = [tf.contrib.rnn.BasicLSTMCell(NUM_HIDDEN_UNITS, forget_bias=1.0) for _ in range(2)]\n",
    "                lstm_layers = tf.contrib.rnn.MultiRNNCell(lstm_layers)\n",
    "\n",
    "                # Get output from LSTM layers\n",
    "                outputs, _ = tf.contrib.rnn.static_rnn(lstm_layers, hidden, dtype=tf.float32)\n",
    "                outputs_test, _ = tf.contrib.rnn.static_rnn(lstm_layers, hidden_test, dtype=tf.float32)\n",
    "\n",
    "                # Get output for the last time step\n",
    "                lstm_last_output = outputs[-1]\n",
    "                lstm_last_output_test = outputs_test[-1]\n",
    "\n",
    "                # Get Logits\n",
    "                logits = tf.matmul(lstm_last_output, W['output']) + biases['output']\n",
    "                logits_test = tf.matmul(lstm_last_output_test, W['output']) + biases['output']\n",
    "\n",
    "                # Make predictions with softmax + argmax\n",
    "                softmax_prediction = tf.nn.softmax(logits, name=\"prediction\")\n",
    "                prediction = tf.argmax(softmax_prediction, 1)\n",
    "                softmax_prediction_test = tf.nn.softmax(logits_test, name=\"prediction_test\")\n",
    "                prediction_test = tf.argmax(softmax_prediction_test, 1)\n",
    "\n",
    "                # L2 Regularization\n",
    "                L2_LOSS = 0.0015\n",
    "                l2 = L2_LOSS * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "\n",
    "                # Cross entropy loss + L2 regularization\n",
    "                loss = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        labels=tf.one_hot(tf.reshape(_y, [-1]), NUM_CLASSES),\n",
    "                        logits=logits))\n",
    "                loss_test = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        labels=tf.one_hot(tf.reshape(y_test_csv, [-1]), NUM_CLASSES),\n",
    "                        logits=logits_test))\n",
    "                loss_reg = loss + l2\n",
    "\n",
    "                # Define optimizer\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "                grads = optimizer.compute_gradients(loss_reg)\n",
    "\n",
    "                # Only first GPU compute accuracy\n",
    "                if i == 0:\n",
    "                    # Test trained model\n",
    "                    correct_prediction = tf.equal(prediction,\n",
    "                                                  tf.argmax(tf.one_hot(tf.reshape(_y, [-1]), NUM_CLASSES), 1))\n",
    "                    correct_prediction_test = tf.equal(prediction_test,\n",
    "                                                       tf.argmax(tf.one_hot(tf.reshape(y_test_csv, [-1]), NUM_CLASSES),\n",
    "                                                                 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "                    accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, tf.float32), name=\"accuracy_test\")\n",
    "\n",
    "                reuse_vars = True\n",
    "                tower_grads.append(grads)\n",
    "\n",
    "            with tf.device('/cpu:0'):\n",
    "                if i == 0:\n",
    "                    tf.summary.scalar(\"loss_test\", loss_test)  # for tensorboard\n",
    "                    tf.summary.scalar(\"acc_test\", accuracy_test)  # for tensorboard\n",
    "\n",
    "        tower_grads = average_gradients(tf, tower_grads)\n",
    "        train_step = optimizer.apply_gradients(tower_grads)\n",
    "\n",
    "        # Utility stuff tensorboard and logging\n",
    "        saver = tf.train.Saver()\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        summary_writer = tf.summary.FileWriter(tensorboard_logdir, graph=tf.get_default_graph())\n",
    "        display_step = 200\n",
    "        history = dict(test_loss=[], test_acc=[], log=[])\n",
    "        \n",
    "        config = tf.ConfigProto(allow_soft_placement = True)\n",
    "        # Run the graph for NUM_STEPS, compute test accuracy incrementally\n",
    "        with tf.Session(config=config) as sess:\n",
    "            print('Initialzing training, first step takes some time to compute the accuracy...')\n",
    "            sess.run(init_op)\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "            step = 0\n",
    "            for i in range(NUM_STEPS):\n",
    "                if coord.should_stop():\n",
    "                    print(\"coord stop\")\n",
    "                    break\n",
    "                else:\n",
    "                    batch_x, batch_y = sess.run([x_train_csv, y_train_csv])\n",
    "                    sess.run(train_step, feed_dict={x_train_csv: batch_x, y_train_csv: batch_y})\n",
    "                    print(\"step: {0}\".format(i))\n",
    "                    #if (i % 50 == 0):\n",
    "                        #print(\"step: {0}\".format(i))\n",
    "                    if (i % display_step == 0):\n",
    "                        eval_x, eval_y = sess.run([x_test_csv, y_test_csv])\n",
    "                        summary, test_a, test_l = sess.run([summary_op, accuracy_test, loss_test],\n",
    "                                                                 feed_dict={x_test_csv: eval_x, y_test_csv: eval_y})\n",
    "                        result = \"step: {0}, test acc: {1}, test loss: {2}\".format(i, test_a, test_l)\n",
    "                        print(result)\n",
    "                        hdfs.log(result)\n",
    "                        history['test_loss'].append(test_a)\n",
    "                        history['test_acc'].append(test_l)\n",
    "                        history['log'].append(result)\n",
    "                        with tf.device('/cpu:0'):\n",
    "                            summary_writer.add_summary(summary, step)\n",
    "                    print(\"\")\n",
    "\n",
    "            endTime = datetime.now()\n",
    "            timeElapsed = endTime - startTime\n",
    "            accs = \"\\n\".join(str(x) for x in history[\"test_acc\"])\n",
    "            loss = \"\\n\".join(str(x) for x in history[\"test_loss\"])\n",
    "            logs = \"\\n\".join(str(x) for x in history[\"log\"])\n",
    "            pyhdfs.dump(accs, OUTPUT_PATH + \"/accuracy\")\n",
    "            pyhdfs.dump(loss, OUTPUT_PATH + \"/loss\")\n",
    "            pyhdfs.dump(logs, OUTPUT_PATH + \"/log\")\n",
    "            time = \"start: \" + str(startTime) + \"\\nend: \" + str(endTime) + \"\\nduration: \" + str(timeElapsed)\n",
    "            pyhdfs.dump(time, OUTPUT_PATH + \"/time\")\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished TensorFlow job \n",
      "\n",
      "Make sure to check /Logs/TensorFlow/application_1513605045578_0477/runId.0 for logfile and TensorBoard logdir"
     ]
    }
   ],
   "source": [
    "from hops import util\n",
    "from hops import tflauncher\n",
    "\n",
    "args_dict = {'learning_rate': [0.0025]}\n",
    "\n",
    "# Generate a grid for the given hyperparameters\n",
    "args_dict_grid = util.grid_params(args_dict)\n",
    "\n",
    "# Launch training\n",
    "tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper, args_dict_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
